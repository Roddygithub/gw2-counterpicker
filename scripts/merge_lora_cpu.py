#!/usr/bin/env python3
"""
Merge LoRA adapters with base model using CPU only
Then convert to GGUF using llama.cpp
"""

import os
import sys
from pathlib import Path

# Paths
SCRIPT_DIR = Path(__file__).parent.parent
LORA_PATH = SCRIPT_DIR / "data" / "qwen25-3b-gw2-lora"
OUTPUT_DIR = SCRIPT_DIR / "data" / "qwen25-3b-gw2-merged"
GGUF_OUTPUT = SCRIPT_DIR / "data" / "qwen25-3b-gw2.gguf"
BASE_MODEL = "Qwen/Qwen2.5-3B-Instruct"

def main():
    print("=" * 60)
    print("üîÑ Merging LoRA adapters with base model (CPU)")
    print("=" * 60)
    
    if not LORA_PATH.exists():
        print(f"‚ùå LoRA path not found: {LORA_PATH}")
        sys.exit(1)
    
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    print(f"üìÅ LoRA path: {LORA_PATH}")
    print(f"üìÅ Output dir: {OUTPUT_DIR}")
    print(f"ü§ñ Base model: {BASE_MODEL}")
    
    # Force CPU
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    import torch
    torch.set_default_device("cpu")
    
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    
    print("\nüì• Loading base model (CPU, this may take a while)...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="cpu",
        low_cpu_mem_usage=True,
    )
    print("‚úì Base model loaded")
    
    print("\nüì• Loading LoRA adapters...")
    model = PeftModel.from_pretrained(base_model, str(LORA_PATH))
    print("‚úì LoRA adapters loaded")
    
    print("\nüîÄ Merging weights...")
    merged_model = model.merge_and_unload()
    print("‚úì Weights merged")
    
    print(f"\nüíæ Saving merged model to {OUTPUT_DIR}...")
    merged_model.save_pretrained(str(OUTPUT_DIR), safe_serialization=True)
    
    tokenizer = AutoTokenizer.from_pretrained(str(LORA_PATH))
    tokenizer.save_pretrained(str(OUTPUT_DIR))
    print("‚úì Merged model saved")
    
    print("\n" + "=" * 60)
    print("‚úÖ MERGE COMPLETE")
    print("=" * 60)
    print(f"\nMerged model saved to: {OUTPUT_DIR}")
    print("\nNext step: Convert to GGUF using llama.cpp:")
    print(f"  python -m llama_cpp.server.convert {OUTPUT_DIR} --outfile {GGUF_OUTPUT} --outtype q4_k_m")

if __name__ == "__main__":
    main()
