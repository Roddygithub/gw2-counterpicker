{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Fine-tuning Qwen2.5:3b pour GW2 WvW Counter-Picker\n",
    "\n",
    "Ce notebook permet de fine-tuner le mod√®le Qwen2.5:3b sur les donn√©es de combats GW2 WvW.\n",
    "\n",
    "**Pr√©requis** :\n",
    "- Google Colab (gratuit)\n",
    "- GPU T4 (activ√© automatiquement)\n",
    "- ~30 minutes pour le fine-tuning\n",
    "\n",
    "**R√©sultat** :\n",
    "- Mod√®le fine-tun√© exportable en GGUF pour Ollama\n",
    "- Meilleure compr√©hension des compositions GW2 WvW\n",
    "- R√©ponses plus pr√©cises au format CONTER/FOCUS/TACTIQUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ V√©rifier le GPU et installer les d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer Unsloth (optimis√© pour le fine-tuning rapide)\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes triton\n",
    "!pip install datasets huggingface_hub\n",
    "\n",
    "print(\"‚úì D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Charger le dataset GW2 WvW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploader le dataset depuis ton PC\n",
    "# Option 1: Upload manuel\n",
    "from google.colab import files\n",
    "print(\"üìÅ Upload le fichier 'finetune_dataset_qwen.jsonl' depuis ton PC:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# V√©rifier le fichier upload√©\n",
    "import os\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"‚úì Fichier upload√©: {filename} ({os.path.getsize(filename)} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et pr√©parer le dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Charger le dataset JSONL\n",
    "dataset = load_dataset(\"json\", data_files=\"finetune_dataset_qwen.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"‚úì Dataset charg√©: {len(dataset)} exemples\")\n",
    "print(f\"\\nüìã Exemple:\")\n",
    "print(f\"Instruction: {dataset[0]['instruction'][:200]}...\")\n",
    "print(f\"Output: {dataset[0]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formater le dataset pour Qwen2.5\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format pour Qwen2.5 chat template\"\"\"\n",
    "    return {\n",
    "        \"text\": f\"\"\"<|im_start|>user\n",
    "{example['instruction']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{example['output']}<|im_end|>\"\"\"\n",
    "    }\n",
    "\n",
    "# Appliquer le formatage\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "print(f\"‚úì Dataset format√© pour Qwen2.5\")\n",
    "print(f\"\\nüìã Exemple format√©:\")\n",
    "print(formatted_dataset[0]['text'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Charger le mod√®le Qwen2.5:3b avec Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Configuration du mod√®le\n",
    "max_seq_length = 2048  # Longueur max des s√©quences\n",
    "dtype = None  # Auto-detect (float16 pour T4)\n",
    "load_in_4bit = True  # Quantification 4-bit pour √©conomiser la VRAM\n",
    "\n",
    "# Charger Qwen2.5:3b\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Mod√®le Qwen2.5-3B-Instruct charg√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les adaptateurs LoRA pour le fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Rang LoRA (16 = bon √©quilibre qualit√©/vitesse)\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Pas de dropout pour plus de stabilit√©\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # √âconomise 30% de VRAM\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Adaptateurs LoRA ajout√©s\")\n",
    "print(f\"‚úì Param√®tres entra√Ænables: {model.print_trainable_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configurer et lancer le fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Pas de packing pour des exemples de longueur variable\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,  # 3 epochs pour un bon apprentissage\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # Pas de logging externe\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer configur√©\")\n",
    "print(f\"‚úì Batch size effectif: {2 * 4} = 8\")\n",
    "print(f\"‚úì Epochs: 3\")\n",
    "print(f\"‚úì Exemples: {len(formatted_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Lancer le fine-tuning\n",
    "print(\"üöÄ D√©marrage du fine-tuning...\")\n",
    "print(\"‚è±Ô∏è Dur√©e estim√©e: 20-30 minutes sur GPU T4\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úì Fine-tuning termin√©!\")\n",
    "print(f\"‚úì Loss finale: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"‚úì Temps total: {trainer_stats.metrics['train_runtime']:.0f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Tester le mod√®le fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer en mode inf√©rence\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test avec une composition ennemie\n",
    "test_prompt = \"\"\"Guild Wars 2 WvW counter-picker.\n",
    "\n",
    "VALID SPECS: Firebrand, Willbender, Dragonhunter, Spellbreaker, Berserker, Bladesworn, Herald, Vindicator, Renegade, Scrapper, Holosmith, Mechanist, Druid, Soulbeast, Untamed, Daredevil, Deadeye, Specter, Tempest, Weaver, Catalyst, Chronomancer, Mirage, Virtuoso, Reaper, Scourge, Harbinger\n",
    "\n",
    "Mode: ZERG (25+ players)\n",
    "Enemy: 4x Firebrand, 3x Scourge, 2x Scrapper, 2x Spellbreaker\n",
    "\n",
    "[ENEMY ANALYSIS]\n",
    "- Firebrand: support, heal, stability (weak to: boon strip, boon corrupt)\n",
    "- Scourge: condi, corrupt, barrier (weak to: burst, focus fire)\n",
    "- Scrapper: support, superspeed, cleanse (weak to: boon strip, focus fire)\n",
    "- Spellbreaker: frontline, strip, cc (weak to: condi pressure, kiting)\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "CONTER: Nx Spec, Nx Spec\n",
    "FOCUS: Target1 > Target2\n",
    "TACTIQUE: One tactical advice\"\"\"\n",
    "\n",
    "# G√©n√©rer la r√©ponse\n",
    "inputs = tokenizer(\n",
    "    f\"<|im_start|>user\\n{test_prompt}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"üìã Test du mod√®le fine-tun√©:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.split(\"assistant\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test suppl√©mentaire - Roaming\n",
    "test_roam = \"\"\"Guild Wars 2 WvW counter-picker.\n",
    "\n",
    "VALID SPECS: Firebrand, Willbender, Dragonhunter, Spellbreaker, Berserker, Bladesworn, Herald, Vindicator, Renegade, Scrapper, Holosmith, Mechanist, Druid, Soulbeast, Untamed, Daredevil, Deadeye, Specter, Tempest, Weaver, Catalyst, Chronomancer, Mirage, Virtuoso, Reaper, Scourge, Harbinger\n",
    "\n",
    "Mode: ROAMING (1-10 players)\n",
    "Enemy: 2x Soulbeast, 1x Deadeye\n",
    "\n",
    "[ENEMY ANALYSIS]\n",
    "- Soulbeast: dps, burst, roam (weak to: CC, sustain fights)\n",
    "- Deadeye: sniper, burst, backline (weak to: mobility, stealth reveal)\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "CONTER: Nx Spec, Nx Spec\n",
    "FOCUS: Target1 > Target2\n",
    "TACTIQUE: One tactical advice\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    f\"<|im_start|>user\\n{test_roam}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"üìã Test Roaming:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.split(\"assistant\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Exporter le mod√®le pour Ollama (GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le LoRA\n",
    "model.save_pretrained(\"qwen25-3b-gw2-lora\")\n",
    "tokenizer.save_pretrained(\"qwen25-3b-gw2-lora\")\n",
    "print(\"‚úì Mod√®le LoRA sauvegard√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter en GGUF pour Ollama (quantification Q4_K_M recommand√©e)\n",
    "# Q4_K_M = bon √©quilibre qualit√©/taille (~2GB)\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    \"qwen25-3b-gw2-gguf\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",  # Quantification 4-bit\n",
    ")\n",
    "\n",
    "print(\"‚úì Mod√®le export√© en GGUF (Q4_K_M)\")\n",
    "print(\"üìÅ Fichier: qwen25-3b-gw2-gguf/unsloth.Q4_K_M.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©charger le mod√®le GGUF\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Trouver le fichier GGUF\n",
    "gguf_dir = \"qwen25-3b-gw2-gguf\"\n",
    "gguf_files = [f for f in os.listdir(gguf_dir) if f.endswith('.gguf')]\n",
    "\n",
    "if gguf_files:\n",
    "    gguf_path = os.path.join(gguf_dir, gguf_files[0])\n",
    "    print(f\"üì• T√©l√©chargement de {gguf_path}...\")\n",
    "    print(f\"   Taille: {os.path.getsize(gguf_path) / 1e9:.2f} GB\")\n",
    "    files.download(gguf_path)\n",
    "else:\n",
    "    print(\"‚ùå Fichier GGUF non trouv√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Instructions pour utiliser le mod√®le avec Ollama\n",
    "\n",
    "Une fois le fichier `.gguf` t√©l√©charg√©, voici comment l'utiliser:\n",
    "\n",
    "### Sur ton serveur:\n",
    "\n",
    "```bash\n",
    "# 1. Copier le fichier GGUF sur le serveur\n",
    "scp unsloth.Q4_K_M.gguf user@server:/path/to/models/\n",
    "\n",
    "# 2. Cr√©er un Modelfile pour Ollama\n",
    "cat > Modelfile << 'EOF'\n",
    "FROM /path/to/models/unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{{ .Response }}<|im_end|>\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.1\n",
    "PARAMETER num_predict 80\n",
    "PARAMETER num_ctx 1024\n",
    "PARAMETER stop \"<|im_end|>\"\n",
    "EOF\n",
    "\n",
    "# 3. Cr√©er le mod√®le Ollama\n",
    "ollama create qwen25-gw2 -f Modelfile\n",
    "\n",
    "# 4. Tester\n",
    "ollama run qwen25-gw2 \"Test prompt...\"\n",
    "```\n",
    "\n",
    "### Dans counter_ai.py:\n",
    "\n",
    "Changer `MODEL_NAME` pour utiliser le mod√®le fine-tun√©:\n",
    "```python\n",
    "MODEL_NAME = \"qwen25-gw2\"  # Mod√®le fine-tun√©\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ R√©sum√©\n",
    "\n",
    "Tu as maintenant:\n",
    "1. ‚úì Fine-tun√© Qwen2.5:3b sur 1537 exemples de combats GW2 WvW\n",
    "2. ‚úì Export√© le mod√®le en GGUF pour Ollama\n",
    "3. ‚úì T√©l√©charg√© le fichier (~2GB)\n",
    "\n",
    "**Prochaines √©tapes:**\n",
    "1. Copier le fichier GGUF sur ton serveur\n",
    "2. Cr√©er le mod√®le Ollama avec le Modelfile\n",
    "3. Mettre √† jour `counter_ai.py` pour utiliser le nouveau mod√®le\n",
    "\n",
    "Le mod√®le fine-tun√© devrait:\n",
    "- Mieux respecter le format CONTER/FOCUS/TACTIQUE\n",
    "- Comprendre les synergies GW2 WvW\n",
    "- Donner des conseils tactiques plus pertinents"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
