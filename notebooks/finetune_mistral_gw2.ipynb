{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Fine-tuning Mistral 7B pour GW2 WvW Counter-Picker\n",
    "\n",
    "Ce notebook permet de fine-tuner le mod√®le Mistral 7B sur les donn√©es de combats GW2 WvW.\n",
    "\n",
    "**Pr√©requis** :\n",
    "- Google Colab (gratuit)\n",
    "- GPU T4 (15GB VRAM - suffisant avec quantification 4-bit)\n",
    "- ~45-60 minutes pour le fine-tuning\n",
    "\n",
    "**Fonctionnalit√©s** :\n",
    "- ‚úÖ Checkpoints automatiques (reprise apr√®s interruption)\n",
    "- ‚úÖ Sauvegarde sur Google Drive\n",
    "- ‚úÖ Export GGUF pour Ollama\n",
    "- ‚úÖ Specs Visions of Eternity incluses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configuration et d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive pour sauvegarder les checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er le dossier de sauvegarde\n",
    "import os\n",
    "SAVE_DIR = \"/content/drive/MyDrive/GW2_FineTuning\"\n",
    "CHECKPOINT_DIR = f\"{SAVE_DIR}/checkpoints_mistral\"\n",
    "GGUF_DIR = f\"{SAVE_DIR}/mistral-7b-gw2-gguf\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(GGUF_DIR, exist_ok=True)\n",
    "print(f\"‚úì Dossier de sauvegarde: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer Unsloth\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes triton\n",
    "!pip install datasets huggingface_hub\n",
    "\n",
    "print(\"‚úì D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Charger le dataset GW2 WvW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploader le dataset depuis ton PC\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# V√©rifier si le dataset existe d√©j√† sur Drive\n",
    "DATASET_PATH = f\"{SAVE_DIR}/finetune_dataset_mistral.jsonl\"\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úì Dataset trouv√© sur Drive: {DATASET_PATH}\")\n",
    "    shutil.copy(DATASET_PATH, \"finetune_dataset_mistral.jsonl\")\n",
    "else:\n",
    "    print(\"üìÅ Upload le fichier 'finetune_dataset_mistral.jsonl' depuis ton PC:\")\n",
    "    uploaded = files.upload()\n",
    "    # Sauvegarder sur Drive pour les prochaines fois\n",
    "    for filename in uploaded.keys():\n",
    "        shutil.copy(filename, DATASET_PATH)\n",
    "        print(f\"‚úì Dataset sauvegard√© sur Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et pr√©parer le dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"finetune_dataset_mistral.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"‚úì Dataset charg√©: {len(dataset)} exemples\")\n",
    "print(f\"\\nüìã Exemple:\")\n",
    "print(f\"Instruction: {dataset[0]['instruction'][:200]}...\")\n",
    "print(f\"Output: {dataset[0]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formater le dataset pour Mistral (format [INST])\n",
    "def format_prompt(example):\n",
    "    instruction = example['instruction']\n",
    "    if not instruction.startswith('[INST]'):\n",
    "        instruction = f\"[INST] {instruction} [/INST]\"\n",
    "    return {\n",
    "        \"text\": f\"{instruction}\\n{example['output']}</s>\"\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "print(f\"‚úì Dataset format√© pour Mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Charger le mod√®le Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True  # IMPORTANT: 4-bit pour tenir dans 15GB VRAM\n",
    "\n",
    "# V√©rifier si un checkpoint existe\n",
    "checkpoint_exists = os.path.exists(f\"{CHECKPOINT_DIR}/checkpoint-latest\")\n",
    "\n",
    "if checkpoint_exists:\n",
    "    print(\"üîÑ Checkpoint trouv√©! Reprise de l'entra√Ænement...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=f\"{CHECKPOINT_DIR}/checkpoint-latest\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "else:\n",
    "    print(\"üì• Chargement du mod√®le Mistral-7B-Instruct-v0.3...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "print(f\"‚úì Mod√®le charg√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les adaptateurs LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Adaptateurs LoRA ajout√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Fine-tuning avec checkpoints automatiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configuration avec checkpoints - batch size r√©duit pour Mistral 7B\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,  # R√©duit pour Mistral 7B\n",
    "        gradient_accumulation_steps=8,  # Compens√© par plus d'accumulation\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=CHECKPOINT_DIR,\n",
    "        # CHECKPOINTS - Sauvegarde toutes les 50 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        resume_from_checkpoint=True if checkpoint_exists else None,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer configur√© avec checkpoints automatiques\")\n",
    "print(f\"‚úì Sauvegarde toutes les 50 steps dans: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Lancer le fine-tuning\n",
    "print(\"üöÄ D√©marrage du fine-tuning Mistral 7B...\")\n",
    "print(\"‚è±Ô∏è Dur√©e estim√©e: 45-60 minutes sur GPU T4\")\n",
    "print(\"üíæ Checkpoints sauvegard√©s sur Google Drive (reprise automatique si interruption)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_exists)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úì Fine-tuning termin√©!\")\n",
    "print(f\"‚úì Loss finale: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le checkpoint final\n",
    "model.save_pretrained(f\"{CHECKPOINT_DIR}/checkpoint-latest\")\n",
    "tokenizer.save_pretrained(f\"{CHECKPOINT_DIR}/checkpoint-latest\")\n",
    "print(f\"‚úì Checkpoint final sauvegard√© sur Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Tester le mod√®le fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = \"\"\"[INST] Guild Wars 2 WvW counter-picker.\n",
    "\n",
    "VALID SPECS: Dragonhunter, Firebrand, Willbender, Luminary, Berserker, Spellbreaker, Bladesworn, Paragon, Scrapper, Holosmith, Mechanist, Amalgam, Druid, Soulbeast, Untamed, Galeshot, Daredevil, Deadeye, Specter, Antiquary, Tempest, Weaver, Catalyst, Evoker, Chronomancer, Mirage, Virtuoso, Troubadour, Reaper, Scourge, Harbinger, Ritualist, Herald, Renegade, Vindicator, Conduit\n",
    "\n",
    "Mode: ZERG (25+ players)\n",
    "Enemy: 4x Firebrand, 3x Scourge, 2x Scrapper, 2x Spellbreaker, 1x Paragon\n",
    "\n",
    "[ENEMY ANALYSIS]\n",
    "- Firebrand: support, heal, stability (weak to: boon strip, boon corrupt)\n",
    "- Scourge: condi, corrupt, barrier (weak to: burst, focus fire)\n",
    "- Scrapper: support, superspeed, cleanse (weak to: boon strip, focus fire)\n",
    "- Spellbreaker: frontline, strip, cc (weak to: condi pressure, kiting)\n",
    "- Paragon: support, chants, stability (weak to: boon corrupt, burst)\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "CONTER: Nx Spec, Nx Spec\n",
    "FOCUS: Target1 > Target2\n",
    "TACTIQUE: One tactical advice [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"üìã Test du mod√®le fine-tun√©:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.split(\"[/INST]\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Exporter en GGUF pour Ollama\n",
    "\n",
    "**Note**: L'export se fait directement sur Google Drive pour √©viter les erreurs d'espace disque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib√©rer de l'espace disque avant l'export\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Nettoyer le cache Colab\n",
    "!rm -rf /root/.cache/huggingface/hub/*\n",
    "!rm -rf /content/sample_data\n",
    "\n",
    "print(\"‚úì Cache nettoy√©\")\n",
    "!df -h /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter en GGUF directement sur Google Drive\n",
    "print(f\"üì¶ Export GGUF vers: {GGUF_DIR}\")\n",
    "print(\"‚è±Ô∏è Cette √©tape peut prendre 10-15 minutes pour Mistral 7B...\")\n",
    "\n",
    "try:\n",
    "    model.save_pretrained_gguf(\n",
    "        GGUF_DIR,\n",
    "        tokenizer,\n",
    "        quantization_method=\"q4_k_m\",\n",
    "    )\n",
    "    print(f\"‚úì Mod√®le export√© en GGUF\")\n",
    "    print(f\"üìÅ Fichier sauvegard√© sur Drive: {GGUF_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur lors de l'export: {e}\")\n",
    "    print(\"\\nüí° Alternative: Sauvegarder les LoRA adapters et merger localement\")\n",
    "    LORA_DIR = f\"{SAVE_DIR}/mistral-7b-gw2-lora\"\n",
    "    model.save_pretrained(LORA_DIR)\n",
    "    tokenizer.save_pretrained(LORA_DIR)\n",
    "    print(f\"‚úì LoRA adapters sauvegard√©s: {LORA_DIR}\")\n",
    "    print(\"\\nPour convertir en GGUF localement:\")\n",
    "    print(\"1. T√©l√©charger le dossier LoRA depuis Google Drive\")\n",
    "    print(\"2. pip install llama-cpp-python\")\n",
    "    print(\"3. python -m llama_cpp.convert --outtype q4_k_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les fichiers export√©s\n",
    "print(\"üìÅ Fichiers sur Google Drive:\")\n",
    "!ls -lh {GGUF_DIR}/ 2>/dev/null || echo \"Pas de fichiers GGUF\"\n",
    "print(\"\\nüìÅ LoRA adapters:\")\n",
    "!ls -lh {SAVE_DIR}/mistral-7b-gw2-lora/ 2>/dev/null || echo \"Pas de LoRA adapters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Instructions pour Ollama\n",
    "\n",
    "### Option A: Avec fichier GGUF\n",
    "```bash\n",
    "# 1. Copier le fichier GGUF sur le serveur/PC\n",
    "scp unsloth.Q4_K_M.gguf user@server:/home/user/models/\n",
    "\n",
    "# 2. Cr√©er le Modelfile\n",
    "cat > Modelfile << 'EOF'\n",
    "FROM /home/user/models/unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"[INST] {{ .Prompt }} [/INST]\n",
    "{{ .Response }}</s>\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.2\n",
    "PARAMETER num_predict 80\n",
    "PARAMETER num_ctx 1024\n",
    "PARAMETER stop \"</s>\"\n",
    "EOF\n",
    "\n",
    "# 3. Cr√©er le mod√®le Ollama\n",
    "ollama create mistral-gw2 -f Modelfile\n",
    "\n",
    "# 4. Tester\n",
    "ollama run mistral-gw2\n",
    "```\n",
    "\n",
    "### Dans counter_ai.py:\n",
    "```python\n",
    "MODEL_NAME = \"mistral-gw2\"  # Mod√®le fine-tun√©\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
