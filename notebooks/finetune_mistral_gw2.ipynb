{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Fine-tuning Mistral 7B pour GW2 WvW Counter-Picker\n",
    "\n",
    "Ce notebook permet de fine-tuner le mod√®le Mistral 7B sur les donn√©es de combats GW2 WvW.\n",
    "\n",
    "**Pr√©requis** :\n",
    "- Google Colab (gratuit)\n",
    "- GPU T4 (15GB VRAM - suffisant avec quantification 4-bit)\n",
    "- ~45-60 minutes pour le fine-tuning\n",
    "\n",
    "**Fonctionnalit√©s** :\n",
    "- ‚úÖ Checkpoints automatiques (reprise apr√®s interruption)\n",
    "- ‚úÖ Sauvegarde sur Google Drive\n",
    "- ‚úÖ Export GGUF pour Ollama\n",
    "\n",
    "**Note** : Mistral 7B est plus gros que Qwen2.5:3b, donc:\n",
    "- Meilleure qualit√© de r√©ponses\n",
    "- Plus lent √† entra√Æner et √† inf√©rer\n",
    "- N√©cessite ~6GB RAM pour tourner sur Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Configuration et d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\n‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monter Google Drive pour sauvegarder les checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er le dossier de sauvegarde\n",
    "import os\n",
    "SAVE_DIR = \"/content/drive/MyDrive/GW2_FineTuning\"\n",
    "CHECKPOINT_DIR = f\"{SAVE_DIR}/checkpoints_mistral\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"‚úì Dossier de sauvegarde: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer Unsloth\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps trl peft accelerate bitsandbytes triton\n",
    "!pip install datasets huggingface_hub\n",
    "\n",
    "print(\"‚úì D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Charger le dataset GW2 WvW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploader le dataset depuis ton PC\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# V√©rifier si le dataset existe d√©j√† sur Drive\n",
    "DATASET_PATH = f\"{SAVE_DIR}/finetune_dataset_mistral.jsonl\"\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úì Dataset trouv√© sur Drive: {DATASET_PATH}\")\n",
    "    shutil.copy(DATASET_PATH, \"finetune_dataset_mistral.jsonl\")\n",
    "else:\n",
    "    print(\"üìÅ Upload le fichier 'finetune_dataset_mistral.jsonl' depuis ton PC:\")\n",
    "    uploaded = files.upload()\n",
    "    # Sauvegarder sur Drive pour les prochaines fois\n",
    "    for filename in uploaded.keys():\n",
    "        shutil.copy(filename, DATASET_PATH)\n",
    "        print(f\"‚úì Dataset sauvegard√© sur Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et pr√©parer le dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"finetune_dataset_mistral.jsonl\", split=\"train\")\n",
    "\n",
    "print(f\"‚úì Dataset charg√©: {len(dataset)} exemples\")\n",
    "print(f\"\\nüìã Exemple:\")\n",
    "print(f\"Instruction: {dataset[0]['instruction'][:200]}...\")\n",
    "print(f\"Output: {dataset[0]['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formater le dataset pour Mistral (format [INST])\n",
    "def format_prompt(example):\n",
    "    # Le dataset Mistral contient d√©j√† le format [INST]...[/INST]\n",
    "    instruction = example['instruction']\n",
    "    # S'assurer que le format est correct\n",
    "    if not instruction.startswith('[INST]'):\n",
    "        instruction = f\"[INST] {instruction} [/INST]\"\n",
    "    return {\n",
    "        \"text\": f\"{instruction}\\n{example['output']}</s>\"\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset.map(format_prompt)\n",
    "print(f\"‚úì Dataset format√© pour Mistral\")\n",
    "print(f\"\\nüìã Exemple format√©:\")\n",
    "print(formatted_dataset[0]['text'][:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Charger le mod√®le Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True  # IMPORTANT: 4-bit pour tenir dans 15GB VRAM\n",
    "\n",
    "# V√©rifier si un checkpoint existe\n",
    "checkpoint_exists = os.path.exists(f\"{CHECKPOINT_DIR}/checkpoint-latest\")\n",
    "\n",
    "if checkpoint_exists:\n",
    "    print(\"üîÑ Checkpoint trouv√©! Reprise de l'entra√Ænement...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=f\"{CHECKPOINT_DIR}/checkpoint-latest\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "else:\n",
    "    print(\"üì• Chargement du mod√®le Mistral-7B-Instruct-v0.3...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "print(f\"‚úì Mod√®le charg√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter les adaptateurs LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Adaptateurs LoRA ajout√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Fine-tuning avec checkpoints automatiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Configuration avec checkpoints - batch size r√©duit pour Mistral 7B\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,  # R√©duit pour Mistral 7B\n",
    "        gradient_accumulation_steps=8,  # Compens√© par plus d'accumulation\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=CHECKPOINT_DIR,\n",
    "        # CHECKPOINTS - Sauvegarde toutes les 50 steps\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        resume_from_checkpoint=True if checkpoint_exists else None,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"‚úì Trainer configur√© avec checkpoints automatiques\")\n",
    "print(f\"‚úì Sauvegarde toutes les 50 steps dans: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Lancer le fine-tuning\n",
    "print(\"üöÄ D√©marrage du fine-tuning Mistral 7B...\")\n",
    "print(\"‚è±Ô∏è Dur√©e estim√©e: 45-60 minutes sur GPU T4\")\n",
    "print(\"üíæ Checkpoints sauvegard√©s sur Google Drive (reprise automatique si interruption)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_exists)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úì Fine-tuning termin√©!\")\n",
    "print(f\"‚úì Loss finale: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le checkpoint final\n",
    "model.save_pretrained(f\"{CHECKPOINT_DIR}/checkpoint-latest\")\n",
    "tokenizer.save_pretrained(f\"{CHECKPOINT_DIR}/checkpoint-latest\")\n",
    "print(f\"‚úì Checkpoint final sauvegard√© sur Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Tester le mod√®le fine-tun√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "test_prompt = \"\"\"[INST] Guild Wars 2 WvW counter-picker.\n",
    "\n",
    "VALID SPECS: Firebrand, Willbender, Dragonhunter, Spellbreaker, Berserker, Bladesworn, Herald, Vindicator, Renegade, Scrapper, Holosmith, Mechanist, Druid, Soulbeast, Untamed, Daredevil, Deadeye, Specter, Tempest, Weaver, Catalyst, Chronomancer, Mirage, Virtuoso, Reaper, Scourge, Harbinger\n",
    "\n",
    "Mode: ZERG (25+ players)\n",
    "Enemy: 4x Firebrand, 3x Scourge, 2x Scrapper, 2x Spellbreaker\n",
    "\n",
    "[ENEMY ANALYSIS]\n",
    "- Firebrand: support, heal, stability (weak to: boon strip, boon corrupt)\n",
    "- Scourge: condi, corrupt, barrier (weak to: burst, focus fire)\n",
    "- Scrapper: support, superspeed, cleanse (weak to: boon strip, focus fire)\n",
    "- Spellbreaker: frontline, strip, cc (weak to: condi pressure, kiting)\n",
    "\n",
    "Respond EXACTLY in this format:\n",
    "CONTER: Nx Spec, Nx Spec\n",
    "FOCUS: Target1 > Target2\n",
    "TACTIQUE: One tactical advice [/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"üìã Test du mod√®le fine-tun√©:\")\n",
    "print(\"=\" * 50)\n",
    "print(response.split(\"[/INST]\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Exporter en GGUF pour Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter en GGUF (Q4_K_M = bon √©quilibre qualit√©/taille)\n",
    "GGUF_DIR = f\"{SAVE_DIR}/mistral-7b-gw2-gguf\"\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    GGUF_DIR,\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "\n",
    "print(f\"‚úì Mod√®le export√© en GGUF\")\n",
    "print(f\"üìÅ Fichier sauvegard√© sur Drive: {GGUF_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√©l√©charger le mod√®le GGUF\n",
    "from google.colab import files\n",
    "\n",
    "gguf_files = [f for f in os.listdir(GGUF_DIR) if f.endswith('.gguf')]\n",
    "if gguf_files:\n",
    "    gguf_path = os.path.join(GGUF_DIR, gguf_files[0])\n",
    "    print(f\"üì• T√©l√©chargement de {gguf_files[0]}...\")\n",
    "    print(f\"   Taille: {os.path.getsize(gguf_path) / 1e9:.2f} GB\")\n",
    "    files.download(gguf_path)\n",
    "else:\n",
    "    print(\"Le fichier GGUF est d√©j√† sur ton Google Drive!\")\n",
    "    print(f\"üìÅ Chemin: {GGUF_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Instructions pour Ollama\n",
    "\n",
    "```bash\n",
    "# 1. Copier le fichier GGUF sur le serveur/PC local\n",
    "scp unsloth.Q4_K_M.gguf user@server:/home/user/models/\n",
    "\n",
    "# 2. Cr√©er le Modelfile\n",
    "cat > Modelfile << 'EOF'\n",
    "FROM /home/user/models/unsloth.Q4_K_M.gguf\n",
    "\n",
    "TEMPLATE \"\"\"[INST] {{ .Prompt }} [/INST]\n",
    "{{ .Response }}</s>\"\"\"\n",
    "\n",
    "PARAMETER temperature 0.2\n",
    "PARAMETER num_predict 80\n",
    "PARAMETER num_ctx 1024\n",
    "PARAMETER stop \"</s>\"\n",
    "EOF\n",
    "\n",
    "# 3. Cr√©er le mod√®le Ollama\n",
    "ollama create mistral-gw2 -f Modelfile\n",
    "\n",
    "# 4. Tester\n",
    "ollama run mistral-gw2\n",
    "```\n",
    "\n",
    "### Dans counter_ai.py:\n",
    "```python\n",
    "MODEL_NAME = \"mistral-gw2\"  # Mod√®le fine-tun√©\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
